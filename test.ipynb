{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df0030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38aea413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Landmark_0</th>\n",
       "      <th>Landmark_1</th>\n",
       "      <th>Landmark_2</th>\n",
       "      <th>Landmark_3</th>\n",
       "      <th>Landmark_4</th>\n",
       "      <th>Landmark_5</th>\n",
       "      <th>Landmark_6</th>\n",
       "      <th>Landmark_7</th>\n",
       "      <th>Landmark_8</th>\n",
       "      <th>Landmark_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Landmark_54</th>\n",
       "      <th>Landmark_55</th>\n",
       "      <th>Landmark_56</th>\n",
       "      <th>Landmark_57</th>\n",
       "      <th>Landmark_58</th>\n",
       "      <th>Landmark_59</th>\n",
       "      <th>Landmark_60</th>\n",
       "      <th>Landmark_61</th>\n",
       "      <th>Landmark_62</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113965</th>\n",
       "      <td>0.553715</td>\n",
       "      <td>0.919008</td>\n",
       "      <td>2.582992e-06</td>\n",
       "      <td>0.418258</td>\n",
       "      <td>0.836995</td>\n",
       "      <td>-0.168586</td>\n",
       "      <td>0.400587</td>\n",
       "      <td>0.694014</td>\n",
       "      <td>-0.243415</td>\n",
       "      <td>0.606387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934492</td>\n",
       "      <td>0.549659</td>\n",
       "      <td>-0.260939</td>\n",
       "      <td>0.882574</td>\n",
       "      <td>0.623661</td>\n",
       "      <td>-0.245335</td>\n",
       "      <td>0.821493</td>\n",
       "      <td>0.668437</td>\n",
       "      <td>-0.199391</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17645</th>\n",
       "      <td>0.457894</td>\n",
       "      <td>0.796323</td>\n",
       "      <td>4.526396e-07</td>\n",
       "      <td>0.409312</td>\n",
       "      <td>0.702999</td>\n",
       "      <td>-0.018500</td>\n",
       "      <td>0.397764</td>\n",
       "      <td>0.596661</td>\n",
       "      <td>-0.030585</td>\n",
       "      <td>0.398318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613716</td>\n",
       "      <td>0.542005</td>\n",
       "      <td>-0.032230</td>\n",
       "      <td>0.623427</td>\n",
       "      <td>0.489198</td>\n",
       "      <td>-0.025614</td>\n",
       "      <td>0.623785</td>\n",
       "      <td>0.453603</td>\n",
       "      <td>-0.018410</td>\n",
       "      <td>আ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129203</th>\n",
       "      <td>0.500207</td>\n",
       "      <td>0.678848</td>\n",
       "      <td>-1.752897e-06</td>\n",
       "      <td>0.374932</td>\n",
       "      <td>0.566436</td>\n",
       "      <td>-0.082771</td>\n",
       "      <td>0.351746</td>\n",
       "      <td>0.442556</td>\n",
       "      <td>-0.155733</td>\n",
       "      <td>0.459590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938802</td>\n",
       "      <td>0.524988</td>\n",
       "      <td>-0.281809</td>\n",
       "      <td>0.793071</td>\n",
       "      <td>0.569643</td>\n",
       "      <td>-0.235592</td>\n",
       "      <td>0.717299</td>\n",
       "      <td>0.586754</td>\n",
       "      <td>-0.167970</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98770</th>\n",
       "      <td>0.276087</td>\n",
       "      <td>0.645267</td>\n",
       "      <td>-7.958430e-07</td>\n",
       "      <td>0.211309</td>\n",
       "      <td>0.625638</td>\n",
       "      <td>-0.016326</td>\n",
       "      <td>0.138466</td>\n",
       "      <td>0.544290</td>\n",
       "      <td>-0.030167</td>\n",
       "      <td>0.100273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326831</td>\n",
       "      <td>0.494618</td>\n",
       "      <td>-0.197612</td>\n",
       "      <td>0.290674</td>\n",
       "      <td>0.551123</td>\n",
       "      <td>-0.170774</td>\n",
       "      <td>0.313423</td>\n",
       "      <td>0.576751</td>\n",
       "      <td>-0.133374</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123885</th>\n",
       "      <td>0.633730</td>\n",
       "      <td>0.924321</td>\n",
       "      <td>3.347989e-06</td>\n",
       "      <td>0.393195</td>\n",
       "      <td>0.836194</td>\n",
       "      <td>-0.100477</td>\n",
       "      <td>0.309537</td>\n",
       "      <td>0.692972</td>\n",
       "      <td>-0.157145</td>\n",
       "      <td>0.424020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767524</td>\n",
       "      <td>0.541025</td>\n",
       "      <td>-0.281115</td>\n",
       "      <td>0.682572</td>\n",
       "      <td>0.619468</td>\n",
       "      <td>-0.243764</td>\n",
       "      <td>0.680631</td>\n",
       "      <td>0.683895</td>\n",
       "      <td>-0.181721</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38668</th>\n",
       "      <td>0.604143</td>\n",
       "      <td>0.797574</td>\n",
       "      <td>-2.482410e-07</td>\n",
       "      <td>0.525292</td>\n",
       "      <td>0.729521</td>\n",
       "      <td>-0.088642</td>\n",
       "      <td>0.469748</td>\n",
       "      <td>0.634560</td>\n",
       "      <td>-0.112006</td>\n",
       "      <td>0.419786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534942</td>\n",
       "      <td>0.472797</td>\n",
       "      <td>0.105274</td>\n",
       "      <td>0.497353</td>\n",
       "      <td>0.462749</td>\n",
       "      <td>0.102667</td>\n",
       "      <td>0.469877</td>\n",
       "      <td>0.461131</td>\n",
       "      <td>0.102363</td>\n",
       "      <td>ঞ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118686</th>\n",
       "      <td>0.563981</td>\n",
       "      <td>0.711570</td>\n",
       "      <td>-2.826766e-06</td>\n",
       "      <td>0.363833</td>\n",
       "      <td>0.579806</td>\n",
       "      <td>-0.080047</td>\n",
       "      <td>0.234669</td>\n",
       "      <td>0.415527</td>\n",
       "      <td>-0.126312</td>\n",
       "      <td>0.201108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609330</td>\n",
       "      <td>0.449529</td>\n",
       "      <td>-0.168072</td>\n",
       "      <td>0.584890</td>\n",
       "      <td>0.527296</td>\n",
       "      <td>-0.145510</td>\n",
       "      <td>0.635324</td>\n",
       "      <td>0.540614</td>\n",
       "      <td>-0.103677</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30785</th>\n",
       "      <td>0.353421</td>\n",
       "      <td>0.823495</td>\n",
       "      <td>-4.867453e-07</td>\n",
       "      <td>0.473873</td>\n",
       "      <td>0.727642</td>\n",
       "      <td>-0.004893</td>\n",
       "      <td>0.538599</td>\n",
       "      <td>0.621460</td>\n",
       "      <td>-0.011340</td>\n",
       "      <td>0.578630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417005</td>\n",
       "      <td>0.496942</td>\n",
       "      <td>-0.083210</td>\n",
       "      <td>0.421688</td>\n",
       "      <td>0.573631</td>\n",
       "      <td>-0.065916</td>\n",
       "      <td>0.376037</td>\n",
       "      <td>0.591574</td>\n",
       "      <td>-0.040380</td>\n",
       "      <td>ঘ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17713</th>\n",
       "      <td>0.456967</td>\n",
       "      <td>0.651965</td>\n",
       "      <td>9.751361e-07</td>\n",
       "      <td>0.376843</td>\n",
       "      <td>0.550991</td>\n",
       "      <td>-0.016066</td>\n",
       "      <td>0.339944</td>\n",
       "      <td>0.451392</td>\n",
       "      <td>-0.029683</td>\n",
       "      <td>0.294251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581010</td>\n",
       "      <td>0.310703</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.591682</td>\n",
       "      <td>0.250542</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.596015</td>\n",
       "      <td>0.201932</td>\n",
       "      <td>0.037681</td>\n",
       "      <td>আ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64873</th>\n",
       "      <td>0.310055</td>\n",
       "      <td>0.851946</td>\n",
       "      <td>2.499566e-07</td>\n",
       "      <td>0.465077</td>\n",
       "      <td>0.763236</td>\n",
       "      <td>-0.026811</td>\n",
       "      <td>0.591965</td>\n",
       "      <td>0.612478</td>\n",
       "      <td>-0.058100</td>\n",
       "      <td>0.689621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238886</td>\n",
       "      <td>0.223626</td>\n",
       "      <td>-0.180214</td>\n",
       "      <td>0.336682</td>\n",
       "      <td>0.169073</td>\n",
       "      <td>-0.196348</td>\n",
       "      <td>0.429591</td>\n",
       "      <td>0.190736</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107554</th>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.746959</td>\n",
       "      <td>-1.106104e-06</td>\n",
       "      <td>0.354069</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>-0.012460</td>\n",
       "      <td>0.229418</td>\n",
       "      <td>0.521206</td>\n",
       "      <td>-0.031884</td>\n",
       "      <td>0.201876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638663</td>\n",
       "      <td>0.219723</td>\n",
       "      <td>-0.108329</td>\n",
       "      <td>0.566976</td>\n",
       "      <td>0.168614</td>\n",
       "      <td>-0.113752</td>\n",
       "      <td>0.500086</td>\n",
       "      <td>0.163597</td>\n",
       "      <td>-0.107115</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36456</th>\n",
       "      <td>0.512062</td>\n",
       "      <td>0.726375</td>\n",
       "      <td>2.270990e-07</td>\n",
       "      <td>0.573962</td>\n",
       "      <td>0.698303</td>\n",
       "      <td>-0.043939</td>\n",
       "      <td>0.612020</td>\n",
       "      <td>0.623729</td>\n",
       "      <td>-0.060273</td>\n",
       "      <td>0.614429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375995</td>\n",
       "      <td>0.504763</td>\n",
       "      <td>-0.044506</td>\n",
       "      <td>0.383159</td>\n",
       "      <td>0.511670</td>\n",
       "      <td>-0.050039</td>\n",
       "      <td>0.403133</td>\n",
       "      <td>0.541529</td>\n",
       "      <td>-0.045889</td>\n",
       "      <td>জ/য</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101419</th>\n",
       "      <td>0.548001</td>\n",
       "      <td>0.698814</td>\n",
       "      <td>-2.089053e-06</td>\n",
       "      <td>0.317411</td>\n",
       "      <td>0.572826</td>\n",
       "      <td>-0.042863</td>\n",
       "      <td>0.301075</td>\n",
       "      <td>0.403597</td>\n",
       "      <td>-0.090216</td>\n",
       "      <td>0.556015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760734</td>\n",
       "      <td>0.330937</td>\n",
       "      <td>-0.288947</td>\n",
       "      <td>0.690324</td>\n",
       "      <td>0.422078</td>\n",
       "      <td>-0.305995</td>\n",
       "      <td>0.688554</td>\n",
       "      <td>0.497382</td>\n",
       "      <td>-0.301292</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70411</th>\n",
       "      <td>0.706270</td>\n",
       "      <td>0.915377</td>\n",
       "      <td>1.177705e-06</td>\n",
       "      <td>0.787469</td>\n",
       "      <td>0.784391</td>\n",
       "      <td>-0.042788</td>\n",
       "      <td>0.814664</td>\n",
       "      <td>0.630316</td>\n",
       "      <td>-0.072468</td>\n",
       "      <td>0.807713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329133</td>\n",
       "      <td>0.537904</td>\n",
       "      <td>-0.147356</td>\n",
       "      <td>0.255275</td>\n",
       "      <td>0.469336</td>\n",
       "      <td>-0.168619</td>\n",
       "      <td>0.198426</td>\n",
       "      <td>0.403197</td>\n",
       "      <td>-0.180528</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34976</th>\n",
       "      <td>0.393141</td>\n",
       "      <td>0.858424</td>\n",
       "      <td>9.783449e-07</td>\n",
       "      <td>0.404487</td>\n",
       "      <td>0.824503</td>\n",
       "      <td>-0.182138</td>\n",
       "      <td>0.380152</td>\n",
       "      <td>0.731508</td>\n",
       "      <td>-0.263213</td>\n",
       "      <td>0.327005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228479</td>\n",
       "      <td>0.474070</td>\n",
       "      <td>0.043546</td>\n",
       "      <td>0.198175</td>\n",
       "      <td>0.416137</td>\n",
       "      <td>0.039966</td>\n",
       "      <td>0.191428</td>\n",
       "      <td>0.364756</td>\n",
       "      <td>0.040169</td>\n",
       "      <td>ছ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>0.447484</td>\n",
       "      <td>0.737863</td>\n",
       "      <td>4.984133e-07</td>\n",
       "      <td>0.560245</td>\n",
       "      <td>0.647578</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.607370</td>\n",
       "      <td>0.528282</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>0.624637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403055</td>\n",
       "      <td>0.297756</td>\n",
       "      <td>0.012871</td>\n",
       "      <td>0.467379</td>\n",
       "      <td>0.278106</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.529448</td>\n",
       "      <td>0.304874</td>\n",
       "      <td>-0.002459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100777</th>\n",
       "      <td>0.643006</td>\n",
       "      <td>0.826116</td>\n",
       "      <td>-5.032270e-07</td>\n",
       "      <td>0.488944</td>\n",
       "      <td>0.774137</td>\n",
       "      <td>-0.055308</td>\n",
       "      <td>0.341831</td>\n",
       "      <td>0.664202</td>\n",
       "      <td>-0.094200</td>\n",
       "      <td>0.215866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786877</td>\n",
       "      <td>0.500722</td>\n",
       "      <td>-0.183229</td>\n",
       "      <td>0.733005</td>\n",
       "      <td>0.579700</td>\n",
       "      <td>-0.148113</td>\n",
       "      <td>0.706191</td>\n",
       "      <td>0.623017</td>\n",
       "      <td>-0.106828</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70272</th>\n",
       "      <td>0.400236</td>\n",
       "      <td>0.897790</td>\n",
       "      <td>6.221391e-07</td>\n",
       "      <td>0.548788</td>\n",
       "      <td>0.848596</td>\n",
       "      <td>-0.069824</td>\n",
       "      <td>0.670067</td>\n",
       "      <td>0.769696</td>\n",
       "      <td>-0.124366</td>\n",
       "      <td>0.757936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253450</td>\n",
       "      <td>0.407093</td>\n",
       "      <td>-0.173383</td>\n",
       "      <td>0.236103</td>\n",
       "      <td>0.317048</td>\n",
       "      <td>-0.184289</td>\n",
       "      <td>0.229344</td>\n",
       "      <td>0.227576</td>\n",
       "      <td>-0.190736</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71166</th>\n",
       "      <td>0.682499</td>\n",
       "      <td>0.634296</td>\n",
       "      <td>-2.110927e-06</td>\n",
       "      <td>0.444749</td>\n",
       "      <td>0.583695</td>\n",
       "      <td>-0.088472</td>\n",
       "      <td>0.262058</td>\n",
       "      <td>0.436518</td>\n",
       "      <td>-0.125605</td>\n",
       "      <td>0.215763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835210</td>\n",
       "      <td>0.290024</td>\n",
       "      <td>-0.179547</td>\n",
       "      <td>0.784834</td>\n",
       "      <td>0.395496</td>\n",
       "      <td>-0.147494</td>\n",
       "      <td>0.766875</td>\n",
       "      <td>0.463690</td>\n",
       "      <td>-0.090686</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88855</th>\n",
       "      <td>0.909438</td>\n",
       "      <td>0.679417</td>\n",
       "      <td>3.213186e-07</td>\n",
       "      <td>0.788314</td>\n",
       "      <td>0.431779</td>\n",
       "      <td>-0.017438</td>\n",
       "      <td>0.609759</td>\n",
       "      <td>0.361669</td>\n",
       "      <td>-0.074131</td>\n",
       "      <td>0.465401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482475</td>\n",
       "      <td>0.809235</td>\n",
       "      <td>-0.243072</td>\n",
       "      <td>0.520971</td>\n",
       "      <td>0.780607</td>\n",
       "      <td>-0.205298</td>\n",
       "      <td>0.582603</td>\n",
       "      <td>0.757713</td>\n",
       "      <td>-0.175960</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31432</th>\n",
       "      <td>0.671591</td>\n",
       "      <td>0.925202</td>\n",
       "      <td>1.543647e-07</td>\n",
       "      <td>0.601927</td>\n",
       "      <td>0.826135</td>\n",
       "      <td>-0.141760</td>\n",
       "      <td>0.553917</td>\n",
       "      <td>0.670480</td>\n",
       "      <td>-0.183985</td>\n",
       "      <td>0.490111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666180</td>\n",
       "      <td>0.473043</td>\n",
       "      <td>0.123075</td>\n",
       "      <td>0.644764</td>\n",
       "      <td>0.407945</td>\n",
       "      <td>0.124015</td>\n",
       "      <td>0.634076</td>\n",
       "      <td>0.348050</td>\n",
       "      <td>0.125508</td>\n",
       "      <td>ঙ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101667</th>\n",
       "      <td>0.636041</td>\n",
       "      <td>0.707971</td>\n",
       "      <td>-1.638671e-06</td>\n",
       "      <td>0.434010</td>\n",
       "      <td>0.602867</td>\n",
       "      <td>-0.085650</td>\n",
       "      <td>0.346091</td>\n",
       "      <td>0.444275</td>\n",
       "      <td>-0.118686</td>\n",
       "      <td>0.417882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668088</td>\n",
       "      <td>0.294158</td>\n",
       "      <td>-0.129139</td>\n",
       "      <td>0.623192</td>\n",
       "      <td>0.384441</td>\n",
       "      <td>-0.115900</td>\n",
       "      <td>0.628150</td>\n",
       "      <td>0.450328</td>\n",
       "      <td>-0.090722</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105979</th>\n",
       "      <td>0.562004</td>\n",
       "      <td>0.683238</td>\n",
       "      <td>-1.041439e-06</td>\n",
       "      <td>0.323976</td>\n",
       "      <td>0.592455</td>\n",
       "      <td>-0.087336</td>\n",
       "      <td>0.249369</td>\n",
       "      <td>0.424188</td>\n",
       "      <td>-0.144936</td>\n",
       "      <td>0.402105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829614</td>\n",
       "      <td>0.357087</td>\n",
       "      <td>-0.311348</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>-0.276162</td>\n",
       "      <td>0.687719</td>\n",
       "      <td>0.513918</td>\n",
       "      <td>-0.221142</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121784</th>\n",
       "      <td>0.506073</td>\n",
       "      <td>0.921683</td>\n",
       "      <td>1.402960e-06</td>\n",
       "      <td>0.266554</td>\n",
       "      <td>0.808680</td>\n",
       "      <td>-0.046800</td>\n",
       "      <td>0.158117</td>\n",
       "      <td>0.678542</td>\n",
       "      <td>-0.097472</td>\n",
       "      <td>0.240328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579057</td>\n",
       "      <td>0.523310</td>\n",
       "      <td>-0.289381</td>\n",
       "      <td>0.506334</td>\n",
       "      <td>0.605247</td>\n",
       "      <td>-0.223558</td>\n",
       "      <td>0.512604</td>\n",
       "      <td>0.674217</td>\n",
       "      <td>-0.146934</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76358</th>\n",
       "      <td>0.671244</td>\n",
       "      <td>0.825121</td>\n",
       "      <td>1.346908e-06</td>\n",
       "      <td>0.459164</td>\n",
       "      <td>0.785328</td>\n",
       "      <td>-0.103211</td>\n",
       "      <td>0.283833</td>\n",
       "      <td>0.723430</td>\n",
       "      <td>-0.169472</td>\n",
       "      <td>0.201921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774232</td>\n",
       "      <td>0.349787</td>\n",
       "      <td>-0.128262</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.278666</td>\n",
       "      <td>-0.137427</td>\n",
       "      <td>0.756582</td>\n",
       "      <td>0.214216</td>\n",
       "      <td>-0.142141</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116028</th>\n",
       "      <td>0.601493</td>\n",
       "      <td>0.969169</td>\n",
       "      <td>2.685315e-06</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.889360</td>\n",
       "      <td>-0.145375</td>\n",
       "      <td>0.259104</td>\n",
       "      <td>0.737241</td>\n",
       "      <td>-0.206985</td>\n",
       "      <td>0.401832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795085</td>\n",
       "      <td>0.618347</td>\n",
       "      <td>-0.348573</td>\n",
       "      <td>0.725726</td>\n",
       "      <td>0.715306</td>\n",
       "      <td>-0.318327</td>\n",
       "      <td>0.694575</td>\n",
       "      <td>0.766484</td>\n",
       "      <td>-0.257207</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112287</th>\n",
       "      <td>0.478228</td>\n",
       "      <td>0.429488</td>\n",
       "      <td>-9.889908e-07</td>\n",
       "      <td>0.334638</td>\n",
       "      <td>0.430979</td>\n",
       "      <td>-0.062949</td>\n",
       "      <td>0.213981</td>\n",
       "      <td>0.463670</td>\n",
       "      <td>-0.127160</td>\n",
       "      <td>0.154842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454827</td>\n",
       "      <td>0.438872</td>\n",
       "      <td>-0.191623</td>\n",
       "      <td>0.444362</td>\n",
       "      <td>0.451765</td>\n",
       "      <td>-0.165935</td>\n",
       "      <td>0.441450</td>\n",
       "      <td>0.413198</td>\n",
       "      <td>-0.144642</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44446</th>\n",
       "      <td>0.587340</td>\n",
       "      <td>0.399633</td>\n",
       "      <td>-1.056741e-08</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.410899</td>\n",
       "      <td>-0.049592</td>\n",
       "      <td>0.603054</td>\n",
       "      <td>0.426569</td>\n",
       "      <td>-0.089688</td>\n",
       "      <td>0.595143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506319</td>\n",
       "      <td>0.428862</td>\n",
       "      <td>-0.102981</td>\n",
       "      <td>0.504436</td>\n",
       "      <td>0.469815</td>\n",
       "      <td>-0.100603</td>\n",
       "      <td>0.509557</td>\n",
       "      <td>0.494355</td>\n",
       "      <td>-0.092979</td>\n",
       "      <td>ঢ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129099</th>\n",
       "      <td>0.788216</td>\n",
       "      <td>0.953045</td>\n",
       "      <td>8.940033e-07</td>\n",
       "      <td>0.618306</td>\n",
       "      <td>0.882928</td>\n",
       "      <td>-0.085104</td>\n",
       "      <td>0.539321</td>\n",
       "      <td>0.762454</td>\n",
       "      <td>-0.094821</td>\n",
       "      <td>0.632066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838095</td>\n",
       "      <td>0.466646</td>\n",
       "      <td>-0.142752</td>\n",
       "      <td>0.819292</td>\n",
       "      <td>0.485433</td>\n",
       "      <td>-0.131230</td>\n",
       "      <td>0.807457</td>\n",
       "      <td>0.541123</td>\n",
       "      <td>-0.124159</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40773</th>\n",
       "      <td>0.518914</td>\n",
       "      <td>0.803689</td>\n",
       "      <td>-6.924163e-10</td>\n",
       "      <td>0.486643</td>\n",
       "      <td>0.742518</td>\n",
       "      <td>-0.063224</td>\n",
       "      <td>0.491706</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>-0.075455</td>\n",
       "      <td>0.507395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585042</td>\n",
       "      <td>0.590115</td>\n",
       "      <td>-0.006005</td>\n",
       "      <td>0.556543</td>\n",
       "      <td>0.592240</td>\n",
       "      <td>-0.010027</td>\n",
       "      <td>0.554698</td>\n",
       "      <td>0.604087</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>ঠ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130693</th>\n",
       "      <td>0.510863</td>\n",
       "      <td>0.717858</td>\n",
       "      <td>-1.996176e-06</td>\n",
       "      <td>0.319218</td>\n",
       "      <td>0.638203</td>\n",
       "      <td>-0.075380</td>\n",
       "      <td>0.199893</td>\n",
       "      <td>0.522483</td>\n",
       "      <td>-0.150282</td>\n",
       "      <td>0.238926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720098</td>\n",
       "      <td>0.561525</td>\n",
       "      <td>-0.294206</td>\n",
       "      <td>0.599869</td>\n",
       "      <td>0.611699</td>\n",
       "      <td>-0.244844</td>\n",
       "      <td>0.577677</td>\n",
       "      <td>0.621890</td>\n",
       "      <td>-0.177354</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>0.502623</td>\n",
       "      <td>0.306130</td>\n",
       "      <td>-3.154242e-07</td>\n",
       "      <td>0.490440</td>\n",
       "      <td>0.455250</td>\n",
       "      <td>-0.068416</td>\n",
       "      <td>0.513970</td>\n",
       "      <td>0.602145</td>\n",
       "      <td>-0.112409</td>\n",
       "      <td>0.540598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473462</td>\n",
       "      <td>0.171028</td>\n",
       "      <td>-0.168764</td>\n",
       "      <td>0.479307</td>\n",
       "      <td>0.180917</td>\n",
       "      <td>-0.167487</td>\n",
       "      <td>0.521447</td>\n",
       "      <td>0.167098</td>\n",
       "      <td>-0.163571</td>\n",
       "      <td>খারাপ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84205</th>\n",
       "      <td>0.603244</td>\n",
       "      <td>0.961995</td>\n",
       "      <td>5.203792e-07</td>\n",
       "      <td>0.382907</td>\n",
       "      <td>0.877516</td>\n",
       "      <td>-0.083249</td>\n",
       "      <td>0.255832</td>\n",
       "      <td>0.743836</td>\n",
       "      <td>-0.138300</td>\n",
       "      <td>0.218204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.476085</td>\n",
       "      <td>-0.196847</td>\n",
       "      <td>0.834406</td>\n",
       "      <td>0.388288</td>\n",
       "      <td>-0.209080</td>\n",
       "      <td>0.842736</td>\n",
       "      <td>0.305038</td>\n",
       "      <td>-0.219426</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106415</th>\n",
       "      <td>0.589853</td>\n",
       "      <td>0.710468</td>\n",
       "      <td>-2.624769e-06</td>\n",
       "      <td>0.343790</td>\n",
       "      <td>0.600763</td>\n",
       "      <td>-0.039040</td>\n",
       "      <td>0.276118</td>\n",
       "      <td>0.431685</td>\n",
       "      <td>-0.057493</td>\n",
       "      <td>0.373549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846356</td>\n",
       "      <td>0.364410</td>\n",
       "      <td>-0.269920</td>\n",
       "      <td>0.747835</td>\n",
       "      <td>0.449069</td>\n",
       "      <td>-0.198579</td>\n",
       "      <td>0.776309</td>\n",
       "      <td>0.489830</td>\n",
       "      <td>-0.110566</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35789</th>\n",
       "      <td>0.469309</td>\n",
       "      <td>0.781965</td>\n",
       "      <td>1.038673e-06</td>\n",
       "      <td>0.570223</td>\n",
       "      <td>0.762835</td>\n",
       "      <td>-0.074403</td>\n",
       "      <td>0.640059</td>\n",
       "      <td>0.712464</td>\n",
       "      <td>-0.112172</td>\n",
       "      <td>0.670303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322062</td>\n",
       "      <td>0.554299</td>\n",
       "      <td>-0.054852</td>\n",
       "      <td>0.325711</td>\n",
       "      <td>0.552925</td>\n",
       "      <td>-0.064047</td>\n",
       "      <td>0.345318</td>\n",
       "      <td>0.578657</td>\n",
       "      <td>-0.063862</td>\n",
       "      <td>জ/য</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34332</th>\n",
       "      <td>0.560709</td>\n",
       "      <td>0.943977</td>\n",
       "      <td>4.984143e-09</td>\n",
       "      <td>0.602589</td>\n",
       "      <td>0.909567</td>\n",
       "      <td>-0.101191</td>\n",
       "      <td>0.614957</td>\n",
       "      <td>0.812663</td>\n",
       "      <td>-0.138093</td>\n",
       "      <td>0.581521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488066</td>\n",
       "      <td>0.574704</td>\n",
       "      <td>-0.025761</td>\n",
       "      <td>0.478417</td>\n",
       "      <td>0.513655</td>\n",
       "      <td>-0.042560</td>\n",
       "      <td>0.480447</td>\n",
       "      <td>0.460914</td>\n",
       "      <td>-0.052962</td>\n",
       "      <td>ছ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106779</th>\n",
       "      <td>0.458387</td>\n",
       "      <td>0.622905</td>\n",
       "      <td>-6.069217e-07</td>\n",
       "      <td>0.368180</td>\n",
       "      <td>0.635163</td>\n",
       "      <td>-0.027546</td>\n",
       "      <td>0.247223</td>\n",
       "      <td>0.564861</td>\n",
       "      <td>-0.043585</td>\n",
       "      <td>0.163510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343186</td>\n",
       "      <td>0.219485</td>\n",
       "      <td>-0.091964</td>\n",
       "      <td>0.269845</td>\n",
       "      <td>0.239062</td>\n",
       "      <td>-0.106477</td>\n",
       "      <td>0.223032</td>\n",
       "      <td>0.289177</td>\n",
       "      <td>-0.107185</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40069</th>\n",
       "      <td>0.510210</td>\n",
       "      <td>0.789387</td>\n",
       "      <td>-7.204567e-07</td>\n",
       "      <td>0.576888</td>\n",
       "      <td>0.735663</td>\n",
       "      <td>-0.053416</td>\n",
       "      <td>0.634887</td>\n",
       "      <td>0.632168</td>\n",
       "      <td>-0.086551</td>\n",
       "      <td>0.647292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375944</td>\n",
       "      <td>0.669954</td>\n",
       "      <td>-0.166411</td>\n",
       "      <td>0.429965</td>\n",
       "      <td>0.754096</td>\n",
       "      <td>-0.150088</td>\n",
       "      <td>0.442272</td>\n",
       "      <td>0.789532</td>\n",
       "      <td>-0.125519</td>\n",
       "      <td>ট</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>0.451032</td>\n",
       "      <td>0.779706</td>\n",
       "      <td>5.223124e-07</td>\n",
       "      <td>0.498437</td>\n",
       "      <td>0.745810</td>\n",
       "      <td>-0.025723</td>\n",
       "      <td>0.524391</td>\n",
       "      <td>0.671776</td>\n",
       "      <td>-0.036106</td>\n",
       "      <td>0.503602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442747</td>\n",
       "      <td>0.583213</td>\n",
       "      <td>-0.075135</td>\n",
       "      <td>0.456149</td>\n",
       "      <td>0.634748</td>\n",
       "      <td>-0.074915</td>\n",
       "      <td>0.460830</td>\n",
       "      <td>0.674432</td>\n",
       "      <td>-0.065323</td>\n",
       "      <td>না</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113430</th>\n",
       "      <td>0.479636</td>\n",
       "      <td>0.839694</td>\n",
       "      <td>1.945358e-06</td>\n",
       "      <td>0.276406</td>\n",
       "      <td>0.739379</td>\n",
       "      <td>-0.065371</td>\n",
       "      <td>0.194994</td>\n",
       "      <td>0.603858</td>\n",
       "      <td>-0.116672</td>\n",
       "      <td>0.294102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670939</td>\n",
       "      <td>0.464088</td>\n",
       "      <td>-0.290253</td>\n",
       "      <td>0.552160</td>\n",
       "      <td>0.538974</td>\n",
       "      <td>-0.264489</td>\n",
       "      <td>0.513608</td>\n",
       "      <td>0.614002</td>\n",
       "      <td>-0.210511</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13200</th>\n",
       "      <td>0.545001</td>\n",
       "      <td>1.047545</td>\n",
       "      <td>1.501533e-06</td>\n",
       "      <td>0.456235</td>\n",
       "      <td>0.940449</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.409887</td>\n",
       "      <td>0.814996</td>\n",
       "      <td>-0.019926</td>\n",
       "      <td>0.385913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629190</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>-0.161644</td>\n",
       "      <td>0.591309</td>\n",
       "      <td>0.691047</td>\n",
       "      <td>-0.134953</td>\n",
       "      <td>0.570975</td>\n",
       "      <td>0.723968</td>\n",
       "      <td>-0.107244</td>\n",
       "      <td>৮</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>0.473743</td>\n",
       "      <td>0.807512</td>\n",
       "      <td>9.654668e-07</td>\n",
       "      <td>0.551992</td>\n",
       "      <td>0.785812</td>\n",
       "      <td>-0.044363</td>\n",
       "      <td>0.584457</td>\n",
       "      <td>0.655066</td>\n",
       "      <td>-0.054215</td>\n",
       "      <td>0.554216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414817</td>\n",
       "      <td>0.599342</td>\n",
       "      <td>-0.124685</td>\n",
       "      <td>0.446459</td>\n",
       "      <td>0.667685</td>\n",
       "      <td>-0.122791</td>\n",
       "      <td>0.463610</td>\n",
       "      <td>0.720255</td>\n",
       "      <td>-0.104283</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50760</th>\n",
       "      <td>0.634878</td>\n",
       "      <td>0.954539</td>\n",
       "      <td>-1.198671e-06</td>\n",
       "      <td>0.753019</td>\n",
       "      <td>0.852720</td>\n",
       "      <td>-0.046076</td>\n",
       "      <td>0.812581</td>\n",
       "      <td>0.684867</td>\n",
       "      <td>-0.058563</td>\n",
       "      <td>0.822704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425566</td>\n",
       "      <td>0.695626</td>\n",
       "      <td>-0.081917</td>\n",
       "      <td>0.493868</td>\n",
       "      <td>0.768961</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.507390</td>\n",
       "      <td>0.798307</td>\n",
       "      <td>-0.014423</td>\n",
       "      <td>ধ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94194</th>\n",
       "      <td>0.569217</td>\n",
       "      <td>0.776177</td>\n",
       "      <td>-1.181223e-06</td>\n",
       "      <td>0.388660</td>\n",
       "      <td>0.708267</td>\n",
       "      <td>-0.072370</td>\n",
       "      <td>0.255909</td>\n",
       "      <td>0.591824</td>\n",
       "      <td>-0.121494</td>\n",
       "      <td>0.199519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804263</td>\n",
       "      <td>0.296384</td>\n",
       "      <td>-0.132141</td>\n",
       "      <td>0.824060</td>\n",
       "      <td>0.209259</td>\n",
       "      <td>-0.158870</td>\n",
       "      <td>0.830313</td>\n",
       "      <td>0.127210</td>\n",
       "      <td>-0.160276</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34721</th>\n",
       "      <td>0.545704</td>\n",
       "      <td>0.844425</td>\n",
       "      <td>7.651082e-08</td>\n",
       "      <td>0.479075</td>\n",
       "      <td>0.805610</td>\n",
       "      <td>-0.137879</td>\n",
       "      <td>0.394564</td>\n",
       "      <td>0.749542</td>\n",
       "      <td>-0.218754</td>\n",
       "      <td>0.288026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381232</td>\n",
       "      <td>0.419973</td>\n",
       "      <td>-0.069660</td>\n",
       "      <td>0.318040</td>\n",
       "      <td>0.398125</td>\n",
       "      <td>-0.082603</td>\n",
       "      <td>0.283098</td>\n",
       "      <td>0.406889</td>\n",
       "      <td>-0.082783</td>\n",
       "      <td>ছ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91311</th>\n",
       "      <td>0.723858</td>\n",
       "      <td>0.594256</td>\n",
       "      <td>3.834600e-07</td>\n",
       "      <td>0.676954</td>\n",
       "      <td>0.408295</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0.538875</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>-0.029873</td>\n",
       "      <td>0.390040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>0.519146</td>\n",
       "      <td>-0.174910</td>\n",
       "      <td>0.374393</td>\n",
       "      <td>0.509148</td>\n",
       "      <td>-0.145068</td>\n",
       "      <td>0.424859</td>\n",
       "      <td>0.516768</td>\n",
       "      <td>-0.119575</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101740</th>\n",
       "      <td>0.444168</td>\n",
       "      <td>0.744148</td>\n",
       "      <td>-1.535732e-06</td>\n",
       "      <td>0.238733</td>\n",
       "      <td>0.597523</td>\n",
       "      <td>-0.009081</td>\n",
       "      <td>0.156310</td>\n",
       "      <td>0.428274</td>\n",
       "      <td>-0.038013</td>\n",
       "      <td>0.171436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360190</td>\n",
       "      <td>0.337436</td>\n",
       "      <td>-0.183734</td>\n",
       "      <td>0.314106</td>\n",
       "      <td>0.434865</td>\n",
       "      <td>-0.167769</td>\n",
       "      <td>0.361896</td>\n",
       "      <td>0.479570</td>\n",
       "      <td>-0.142946</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>0.694283</td>\n",
       "      <td>0.817948</td>\n",
       "      <td>-8.186742e-07</td>\n",
       "      <td>0.767254</td>\n",
       "      <td>0.705719</td>\n",
       "      <td>-0.090394</td>\n",
       "      <td>0.760612</td>\n",
       "      <td>0.575739</td>\n",
       "      <td>-0.137663</td>\n",
       "      <td>0.764347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529350</td>\n",
       "      <td>0.815649</td>\n",
       "      <td>-0.094629</td>\n",
       "      <td>0.582442</td>\n",
       "      <td>0.832063</td>\n",
       "      <td>-0.082370</td>\n",
       "      <td>0.595023</td>\n",
       "      <td>0.821320</td>\n",
       "      <td>-0.069655</td>\n",
       "      <td>৬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56527</th>\n",
       "      <td>0.733759</td>\n",
       "      <td>0.696920</td>\n",
       "      <td>1.052797e-06</td>\n",
       "      <td>0.744641</td>\n",
       "      <td>0.582158</td>\n",
       "      <td>-0.063694</td>\n",
       "      <td>0.716420</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>-0.090305</td>\n",
       "      <td>0.655308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396463</td>\n",
       "      <td>0.453481</td>\n",
       "      <td>-0.066354</td>\n",
       "      <td>0.365228</td>\n",
       "      <td>0.418556</td>\n",
       "      <td>-0.085071</td>\n",
       "      <td>0.342014</td>\n",
       "      <td>0.387543</td>\n",
       "      <td>-0.096746</td>\n",
       "      <td>ম</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15353</th>\n",
       "      <td>0.490907</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>5.563073e-07</td>\n",
       "      <td>0.423421</td>\n",
       "      <td>0.666596</td>\n",
       "      <td>-0.049208</td>\n",
       "      <td>0.400339</td>\n",
       "      <td>0.573448</td>\n",
       "      <td>-0.062492</td>\n",
       "      <td>0.394124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557193</td>\n",
       "      <td>0.472974</td>\n",
       "      <td>0.028913</td>\n",
       "      <td>0.546338</td>\n",
       "      <td>0.422587</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>0.533336</td>\n",
       "      <td>0.386452</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>অ/য়</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Landmark_0  Landmark_1    Landmark_2  Landmark_3  Landmark_4  \\\n",
       "113965    0.553715    0.919008  2.582992e-06    0.418258    0.836995   \n",
       "17645     0.457894    0.796323  4.526396e-07    0.409312    0.702999   \n",
       "129203    0.500207    0.678848 -1.752897e-06    0.374932    0.566436   \n",
       "98770     0.276087    0.645267 -7.958430e-07    0.211309    0.625638   \n",
       "123885    0.633730    0.924321  3.347989e-06    0.393195    0.836194   \n",
       "38668     0.604143    0.797574 -2.482410e-07    0.525292    0.729521   \n",
       "118686    0.563981    0.711570 -2.826766e-06    0.363833    0.579806   \n",
       "30785     0.353421    0.823495 -4.867453e-07    0.473873    0.727642   \n",
       "17713     0.456967    0.651965  9.751361e-07    0.376843    0.550991   \n",
       "64873     0.310055    0.851946  2.499566e-07    0.465077    0.763236   \n",
       "107554    0.552239    0.746959 -1.106104e-06    0.354069    0.663707   \n",
       "36456     0.512062    0.726375  2.270990e-07    0.573962    0.698303   \n",
       "101419    0.548001    0.698814 -2.089053e-06    0.317411    0.572826   \n",
       "70411     0.706270    0.915377  1.177705e-06    0.787469    0.784391   \n",
       "34976     0.393141    0.858424  9.783449e-07    0.404487    0.824503   \n",
       "1281      0.447484    0.737863  4.984133e-07    0.560245    0.647578   \n",
       "100777    0.643006    0.826116 -5.032270e-07    0.488944    0.774137   \n",
       "70272     0.400236    0.897790  6.221391e-07    0.548788    0.848596   \n",
       "71166     0.682499    0.634296 -2.110927e-06    0.444749    0.583695   \n",
       "88855     0.909438    0.679417  3.213186e-07    0.788314    0.431779   \n",
       "31432     0.671591    0.925202  1.543647e-07    0.601927    0.826135   \n",
       "101667    0.636041    0.707971 -1.638671e-06    0.434010    0.602867   \n",
       "105979    0.562004    0.683238 -1.041439e-06    0.323976    0.592455   \n",
       "121784    0.506073    0.921683  1.402960e-06    0.266554    0.808680   \n",
       "76358     0.671244    0.825121  1.346908e-06    0.459164    0.785328   \n",
       "116028    0.601493    0.969169  2.685315e-06    0.361100    0.889360   \n",
       "112287    0.478228    0.429488 -9.889908e-07    0.334638    0.430979   \n",
       "44446     0.587340    0.399633 -1.056741e-08    0.600877    0.410899   \n",
       "129099    0.788216    0.953045  8.940033e-07    0.618306    0.882928   \n",
       "40773     0.518914    0.803689 -6.924163e-10    0.486643    0.742518   \n",
       "130693    0.510863    0.717858 -1.996176e-06    0.319218    0.638203   \n",
       "637       0.502623    0.306130 -3.154242e-07    0.490440    0.455250   \n",
       "84205     0.603244    0.961995  5.203792e-07    0.382907    0.877516   \n",
       "106415    0.589853    0.710468 -2.624769e-06    0.343790    0.600763   \n",
       "35789     0.469309    0.781965  1.038673e-06    0.570223    0.762835   \n",
       "34332     0.560709    0.943977  4.984143e-09    0.602589    0.909567   \n",
       "106779    0.458387    0.622905 -6.069217e-07    0.368180    0.635163   \n",
       "40069     0.510210    0.789387 -7.204567e-07    0.576888    0.735663   \n",
       "816       0.451032    0.779706  5.223124e-07    0.498437    0.745810   \n",
       "113430    0.479636    0.839694  1.945358e-06    0.276406    0.739379   \n",
       "13200     0.545001    1.047545  1.501533e-06    0.456235    0.940449   \n",
       "4208      0.473743    0.807512  9.654668e-07    0.551992    0.785812   \n",
       "50760     0.634878    0.954539 -1.198671e-06    0.753019    0.852720   \n",
       "94194     0.569217    0.776177 -1.181223e-06    0.388660    0.708267   \n",
       "34721     0.545704    0.844425  7.651082e-08    0.479075    0.805610   \n",
       "91311     0.723858    0.594256  3.834600e-07    0.676954    0.408295   \n",
       "101740    0.444168    0.744148 -1.535732e-06    0.238733    0.597523   \n",
       "10020     0.694283    0.817948 -8.186742e-07    0.767254    0.705719   \n",
       "56527     0.733759    0.696920  1.052797e-06    0.744641    0.582158   \n",
       "15353     0.490907    0.716845  5.563073e-07    0.423421    0.666596   \n",
       "\n",
       "        Landmark_5  Landmark_6  Landmark_7  Landmark_8  Landmark_9  ...  \\\n",
       "113965   -0.168586    0.400587    0.694014   -0.243415    0.606387  ...   \n",
       "17645    -0.018500    0.397764    0.596661   -0.030585    0.398318  ...   \n",
       "129203   -0.082771    0.351746    0.442556   -0.155733    0.459590  ...   \n",
       "98770    -0.016326    0.138466    0.544290   -0.030167    0.100273  ...   \n",
       "123885   -0.100477    0.309537    0.692972   -0.157145    0.424020  ...   \n",
       "38668    -0.088642    0.469748    0.634560   -0.112006    0.419786  ...   \n",
       "118686   -0.080047    0.234669    0.415527   -0.126312    0.201108  ...   \n",
       "30785    -0.004893    0.538599    0.621460   -0.011340    0.578630  ...   \n",
       "17713    -0.016066    0.339944    0.451392   -0.029683    0.294251  ...   \n",
       "64873    -0.026811    0.591965    0.612478   -0.058100    0.689621  ...   \n",
       "107554   -0.012460    0.229418    0.521206   -0.031884    0.201876  ...   \n",
       "36456    -0.043939    0.612020    0.623729   -0.060273    0.614429  ...   \n",
       "101419   -0.042863    0.301075    0.403597   -0.090216    0.556015  ...   \n",
       "70411    -0.042788    0.814664    0.630316   -0.072468    0.807713  ...   \n",
       "34976    -0.182138    0.380152    0.731508   -0.263213    0.327005  ...   \n",
       "1281      0.009509    0.607370    0.528282    0.028336    0.624637  ...   \n",
       "100777   -0.055308    0.341831    0.664202   -0.094200    0.215866  ...   \n",
       "70272    -0.069824    0.670067    0.769696   -0.124366    0.757936  ...   \n",
       "71166    -0.088472    0.262058    0.436518   -0.125605    0.215763  ...   \n",
       "88855    -0.017438    0.609759    0.361669   -0.074131    0.465401  ...   \n",
       "31432    -0.141760    0.553917    0.670480   -0.183985    0.490111  ...   \n",
       "101667   -0.085650    0.346091    0.444275   -0.118686    0.417882  ...   \n",
       "105979   -0.087336    0.249369    0.424188   -0.144936    0.402105  ...   \n",
       "121784   -0.046800    0.158117    0.678542   -0.097472    0.240328  ...   \n",
       "76358    -0.103211    0.283833    0.723430   -0.169472    0.201921  ...   \n",
       "116028   -0.145375    0.259104    0.737241   -0.206985    0.401832  ...   \n",
       "112287   -0.062949    0.213981    0.463670   -0.127160    0.154842  ...   \n",
       "44446    -0.049592    0.603054    0.426569   -0.089688    0.595143  ...   \n",
       "129099   -0.085104    0.539321    0.762454   -0.094821    0.632066  ...   \n",
       "40773    -0.063224    0.491706    0.639200   -0.075455    0.507395  ...   \n",
       "130693   -0.075380    0.199893    0.522483   -0.150282    0.238926  ...   \n",
       "637      -0.068416    0.513970    0.602145   -0.112409    0.540598  ...   \n",
       "84205    -0.083249    0.255832    0.743836   -0.138300    0.218204  ...   \n",
       "106415   -0.039040    0.276118    0.431685   -0.057493    0.373549  ...   \n",
       "35789    -0.074403    0.640059    0.712464   -0.112172    0.670303  ...   \n",
       "34332    -0.101191    0.614957    0.812663   -0.138093    0.581521  ...   \n",
       "106779   -0.027546    0.247223    0.564861   -0.043585    0.163510  ...   \n",
       "40069    -0.053416    0.634887    0.632168   -0.086551    0.647292  ...   \n",
       "816      -0.025723    0.524391    0.671776   -0.036106    0.503602  ...   \n",
       "113430   -0.065371    0.194994    0.603858   -0.116672    0.294102  ...   \n",
       "13200     0.001404    0.409887    0.814996   -0.019926    0.385913  ...   \n",
       "4208     -0.044363    0.584457    0.655066   -0.054215    0.554216  ...   \n",
       "50760    -0.046076    0.812581    0.684867   -0.058563    0.822704  ...   \n",
       "94194    -0.072370    0.255909    0.591824   -0.121494    0.199519  ...   \n",
       "34721    -0.137879    0.394564    0.749542   -0.218754    0.288026  ...   \n",
       "91311    -0.000268    0.538875    0.298932   -0.029873    0.390040  ...   \n",
       "101740   -0.009081    0.156310    0.428274   -0.038013    0.171436  ...   \n",
       "10020    -0.090394    0.760612    0.575739   -0.137663    0.764347  ...   \n",
       "56527    -0.063694    0.716420    0.477500   -0.090305    0.655308  ...   \n",
       "15353    -0.049208    0.400339    0.573448   -0.062492    0.394124  ...   \n",
       "\n",
       "        Landmark_54  Landmark_55  Landmark_56  Landmark_57  Landmark_58  \\\n",
       "113965     0.934492     0.549659    -0.260939     0.882574     0.623661   \n",
       "17645      0.613716     0.542005    -0.032230     0.623427     0.489198   \n",
       "129203     0.938802     0.524988    -0.281809     0.793071     0.569643   \n",
       "98770      0.326831     0.494618    -0.197612     0.290674     0.551123   \n",
       "123885     0.767524     0.541025    -0.281115     0.682572     0.619468   \n",
       "38668      0.534942     0.472797     0.105274     0.497353     0.462749   \n",
       "118686     0.609330     0.449529    -0.168072     0.584890     0.527296   \n",
       "30785      0.417005     0.496942    -0.083210     0.421688     0.573631   \n",
       "17713      0.581010     0.310703     0.017708     0.591682     0.250542   \n",
       "64873      0.238886     0.223626    -0.180214     0.336682     0.169073   \n",
       "107554     0.638663     0.219723    -0.108329     0.566976     0.168614   \n",
       "36456      0.375995     0.504763    -0.044506     0.383159     0.511670   \n",
       "101419     0.760734     0.330937    -0.288947     0.690324     0.422078   \n",
       "70411      0.329133     0.537904    -0.147356     0.255275     0.469336   \n",
       "34976      0.228479     0.474070     0.043546     0.198175     0.416137   \n",
       "1281       0.403055     0.297756     0.012871     0.467379     0.278106   \n",
       "100777     0.786877     0.500722    -0.183229     0.733005     0.579700   \n",
       "70272      0.253450     0.407093    -0.173383     0.236103     0.317048   \n",
       "71166      0.835210     0.290024    -0.179547     0.784834     0.395496   \n",
       "88855      0.482475     0.809235    -0.243072     0.520971     0.780607   \n",
       "31432      0.666180     0.473043     0.123075     0.644764     0.407945   \n",
       "101667     0.668088     0.294158    -0.129139     0.623192     0.384441   \n",
       "105979     0.829614     0.357087    -0.311348     0.735772     0.444558   \n",
       "121784     0.579057     0.523310    -0.289381     0.506334     0.605247   \n",
       "76358      0.774232     0.349787    -0.128262     0.761488     0.278666   \n",
       "116028     0.795085     0.618347    -0.348573     0.725726     0.715306   \n",
       "112287     0.454827     0.438872    -0.191623     0.444362     0.451765   \n",
       "44446      0.506319     0.428862    -0.102981     0.504436     0.469815   \n",
       "129099     0.838095     0.466646    -0.142752     0.819292     0.485433   \n",
       "40773      0.585042     0.590115    -0.006005     0.556543     0.592240   \n",
       "130693     0.720098     0.561525    -0.294206     0.599869     0.611699   \n",
       "637        0.473462     0.171028    -0.168764     0.479307     0.180917   \n",
       "84205      0.822430     0.476085    -0.196847     0.834406     0.388288   \n",
       "106415     0.846356     0.364410    -0.269920     0.747835     0.449069   \n",
       "35789      0.322062     0.554299    -0.054852     0.325711     0.552925   \n",
       "34332      0.488066     0.574704    -0.025761     0.478417     0.513655   \n",
       "106779     0.343186     0.219485    -0.091964     0.269845     0.239062   \n",
       "40069      0.375944     0.669954    -0.166411     0.429965     0.754096   \n",
       "816        0.442747     0.583213    -0.075135     0.456149     0.634748   \n",
       "113430     0.670939     0.464088    -0.290253     0.552160     0.538974   \n",
       "13200      0.629190     0.676190    -0.161644     0.591309     0.691047   \n",
       "4208       0.414817     0.599342    -0.124685     0.446459     0.667685   \n",
       "50760      0.425566     0.695626    -0.081917     0.493868     0.768961   \n",
       "94194      0.804263     0.296384    -0.132141     0.824060     0.209259   \n",
       "34721      0.381232     0.419973    -0.069660     0.318040     0.398125   \n",
       "91311      0.332005     0.519146    -0.174910     0.374393     0.509148   \n",
       "101740     0.360190     0.337436    -0.183734     0.314106     0.434865   \n",
       "10020      0.529350     0.815649    -0.094629     0.582442     0.832063   \n",
       "56527      0.396463     0.453481    -0.066354     0.365228     0.418556   \n",
       "15353      0.557193     0.472974     0.028913     0.546338     0.422587   \n",
       "\n",
       "        Landmark_59  Landmark_60  Landmark_61  Landmark_62  Label  \n",
       "113965    -0.245335     0.821493     0.668437    -0.199391      R  \n",
       "17645     -0.025614     0.623785     0.453603    -0.018410      আ  \n",
       "129203    -0.235592     0.717299     0.586754    -0.167970      X  \n",
       "98770     -0.170774     0.313423     0.576751    -0.133374      L  \n",
       "123885    -0.243764     0.680631     0.683895    -0.181721      V  \n",
       "38668      0.102667     0.469877     0.461131     0.102363      ঞ  \n",
       "118686    -0.145510     0.635324     0.540614    -0.103677      T  \n",
       "30785     -0.065916     0.376037     0.591574    -0.040380      ঘ  \n",
       "17713      0.029014     0.596015     0.201932     0.037681      আ  \n",
       "64873     -0.196348     0.429591     0.190736    -0.191667      0  \n",
       "107554    -0.113752     0.500086     0.163597    -0.107115      O  \n",
       "36456     -0.050039     0.403133     0.541529    -0.045889    জ/য  \n",
       "101419    -0.305995     0.688554     0.497382    -0.301292      M  \n",
       "70411     -0.168619     0.198426     0.403197    -0.180528      9  \n",
       "34976      0.039966     0.191428     0.364756     0.040169      ছ  \n",
       "1281       0.001405     0.529448     0.304874    -0.002459      0  \n",
       "100777    -0.148113     0.706191     0.623017    -0.106828      L  \n",
       "70272     -0.184289     0.229344     0.227576    -0.190736      9  \n",
       "71166     -0.147494     0.766875     0.463690    -0.090686      A  \n",
       "88855     -0.205298     0.582603     0.757713    -0.175960      H  \n",
       "31432      0.124015     0.634076     0.348050     0.125508      ঙ  \n",
       "101667    -0.115900     0.628150     0.450328    -0.090722      M  \n",
       "105979    -0.276162     0.687719     0.513918    -0.221142      N  \n",
       "121784    -0.223558     0.512604     0.674217    -0.146934      U  \n",
       "76358     -0.137427     0.756582     0.214216    -0.142141      C  \n",
       "116028    -0.318327     0.694575     0.766484    -0.257207      R  \n",
       "112287    -0.165935     0.441450     0.413198    -0.144642      Q  \n",
       "44446     -0.100603     0.509557     0.494355    -0.092979      ঢ  \n",
       "129099    -0.131230     0.807457     0.541123    -0.124159      W  \n",
       "40773     -0.010027     0.554698     0.604087    -0.007086      ঠ  \n",
       "130693    -0.244844     0.577677     0.621890    -0.177354      X  \n",
       "637       -0.167487     0.521447     0.167098    -0.163571  খারাপ  \n",
       "84205     -0.209080     0.842736     0.305038    -0.219426      F  \n",
       "106415    -0.198579     0.776309     0.489830    -0.110566      N  \n",
       "35789     -0.064047     0.345318     0.578657    -0.063862    জ/য  \n",
       "34332     -0.042560     0.480447     0.460914    -0.052962      ছ  \n",
       "106779    -0.106477     0.223032     0.289177    -0.107185      O  \n",
       "40069     -0.150088     0.442272     0.789532    -0.125519      ট  \n",
       "816       -0.074915     0.460830     0.674432    -0.065323     না  \n",
       "113430    -0.264489     0.513608     0.614002    -0.210511      R  \n",
       "13200     -0.134953     0.570975     0.723968    -0.107244      ৮  \n",
       "4208      -0.122791     0.463610     0.720255    -0.104283      2  \n",
       "50760     -0.051782     0.507390     0.798307    -0.014423      ধ  \n",
       "94194     -0.158870     0.830313     0.127210    -0.160276      I  \n",
       "34721     -0.082603     0.283098     0.406889    -0.082783      ছ  \n",
       "91311     -0.145068     0.424859     0.516768    -0.119575      H  \n",
       "101740    -0.167769     0.361896     0.479570    -0.142946      M  \n",
       "10020     -0.082370     0.595023     0.821320    -0.069655      ৬  \n",
       "56527     -0.085071     0.342014     0.387543    -0.096746      ম  \n",
       "15353      0.027890     0.533336     0.386452     0.028481    অ/য়  \n",
       "\n",
       "[50 rows x 64 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"sign_language_data.csv\")\n",
    "df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131a0d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Landmark_0', 'Landmark_1', 'Landmark_2', 'Landmark_3', 'Landmark_4',\n",
       "       'Landmark_5', 'Landmark_6', 'Landmark_7', 'Landmark_8', 'Landmark_9',\n",
       "       'Landmark_10', 'Landmark_11', 'Landmark_12', 'Landmark_13',\n",
       "       'Landmark_14', 'Landmark_15', 'Landmark_16', 'Landmark_17',\n",
       "       'Landmark_18', 'Landmark_19', 'Landmark_20', 'Landmark_21',\n",
       "       'Landmark_22', 'Landmark_23', 'Landmark_24', 'Landmark_25',\n",
       "       'Landmark_26', 'Landmark_27', 'Landmark_28', 'Landmark_29',\n",
       "       'Landmark_30', 'Landmark_31', 'Landmark_32', 'Landmark_33',\n",
       "       'Landmark_34', 'Landmark_35', 'Landmark_36', 'Landmark_37',\n",
       "       'Landmark_38', 'Landmark_39', 'Landmark_40', 'Landmark_41',\n",
       "       'Landmark_42', 'Landmark_43', 'Landmark_44', 'Landmark_45',\n",
       "       'Landmark_46', 'Landmark_47', 'Landmark_48', 'Landmark_49',\n",
       "       'Landmark_50', 'Landmark_51', 'Landmark_52', 'Landmark_53',\n",
       "       'Landmark_54', 'Landmark_55', 'Landmark_56', 'Landmark_57',\n",
       "       'Landmark_58', 'Landmark_59', 'Landmark_60', 'Landmark_61',\n",
       "       'Landmark_62', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92570ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Landmark_0     0\n",
       "Landmark_1     0\n",
       "Landmark_2     0\n",
       "Landmark_3     0\n",
       "Landmark_4     0\n",
       "              ..\n",
       "Landmark_59    0\n",
       "Landmark_60    0\n",
       "Landmark_61    0\n",
       "Landmark_62    0\n",
       "Label          0\n",
       "Length: 64, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ab8a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Landmark_0     float64\n",
       "Landmark_1     float64\n",
       "Landmark_2     float64\n",
       "Landmark_3     float64\n",
       "Landmark_4     float64\n",
       "                ...   \n",
       "Landmark_59    float64\n",
       "Landmark_60    float64\n",
       "Landmark_61    float64\n",
       "Landmark_62    float64\n",
       "Label           object\n",
       "Length: 64, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce46a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                count          mean       std       min           25%  \\\n",
      "Landmark_0   135114.0  5.465449e-01  0.117072  0.059651  4.742509e-01   \n",
      "Landmark_1   135114.0  7.558576e-01  0.159483 -0.193644  6.805722e-01   \n",
      "Landmark_2   135114.0 -1.384115e-08  0.000001 -0.000004 -7.682301e-07   \n",
      "Landmark_3   135114.0  4.719244e-01  0.142776  0.030234  3.586554e-01   \n",
      "Landmark_4   135114.0  6.759015e-01  0.149140 -0.056452  5.888997e-01   \n",
      "Landmark_5   135114.0 -4.993361e-02  0.054989 -0.432685 -7.792687e-02   \n",
      "Landmark_6   135114.0  4.214957e-01  0.177728 -0.036066  2.660235e-01   \n",
      "Landmark_7   135114.0  5.661676e-01  0.135843  0.013715  4.617525e-01   \n",
      "Landmark_8   135114.0 -7.613147e-02  0.079712 -0.618936 -1.177885e-01   \n",
      "Landmark_9   135114.0  4.115132e-01  0.181822 -0.147966  2.595684e-01   \n",
      "Landmark_10  135114.0  4.821871e-01  0.134308 -0.296358  3.717267e-01   \n",
      "Landmark_11  135114.0 -1.025564e-01  0.100928 -0.735915 -1.561277e-01   \n",
      "Landmark_12  135114.0  4.252932e-01  0.189516 -0.221557  2.839294e-01   \n",
      "Landmark_13  135114.0  4.310356e-01  0.145388 -0.498765  3.276124e-01   \n",
      "Landmark_14  135114.0 -1.238274e-01  0.123645 -0.843559 -1.881337e-01   \n",
      "Landmark_15  135114.0  4.567626e-01  0.136922  0.024500  3.507616e-01   \n",
      "Landmark_16  135114.0  4.441145e-01  0.120360 -0.178231  3.472717e-01   \n",
      "Landmark_17  135114.0 -2.909231e-02  0.077376 -0.740458 -6.962493e-02   \n",
      "Landmark_18  135114.0  4.234375e-01  0.151947 -0.066201  3.047290e-01   \n",
      "Landmark_19  135114.0  3.375884e-01  0.116193 -0.282897  2.474859e-01   \n",
      "Landmark_20  135114.0 -8.277226e-02  0.101178 -0.968925 -1.322204e-01   \n",
      "Landmark_21  135114.0  4.058890e-01  0.167326 -0.124385  2.679751e-01   \n",
      "Landmark_22  135114.0  3.187345e-01  0.120646 -0.607884  2.336356e-01   \n",
      "Landmark_23  135114.0 -1.185906e-01  0.122129 -1.100616 -1.826155e-01   \n",
      "Landmark_24  135114.0  3.987308e-01  0.181305 -0.194258  2.533163e-01   \n",
      "Landmark_25  135114.0  3.101500e-01  0.150599 -0.749076  1.951719e-01   \n",
      "Landmark_26  135114.0 -1.378569e-01  0.135901 -1.190648 -2.105625e-01   \n",
      "Landmark_27  135114.0  5.010882e-01  0.107580 -0.008921  4.311391e-01   \n",
      "Landmark_28  135114.0  4.520874e-01  0.116761 -0.258728  3.587289e-01   \n",
      "Landmark_29  135114.0 -3.448722e-02  0.066959 -0.646614 -6.897606e-02   \n",
      "Landmark_30  135114.0  4.671974e-01  0.124783 -0.092252  3.840272e-01   \n",
      "Landmark_31  135114.0  3.531031e-01  0.120669 -0.373282  2.872984e-01   \n",
      "Landmark_32  135114.0 -9.226868e-02  0.105732 -0.786337 -1.495725e-01   \n",
      "Landmark_33  135114.0  4.489013e-01  0.138435 -0.112462  3.593742e-01   \n",
      "Landmark_34  135114.0  3.623233e-01  0.137164 -0.546893  2.556239e-01   \n",
      "Landmark_35  135114.0 -1.188579e-01  0.122182 -0.869104 -1.870868e-01   \n",
      "Landmark_36  135114.0  4.441941e-01  0.155355 -0.221038  3.496419e-01   \n",
      "Landmark_37  135114.0  3.687045e-01  0.174530 -0.513731  2.180575e-01   \n",
      "Landmark_38  135114.0 -1.258925e-01  0.128785 -0.887206 -1.975965e-01   \n",
      "Landmark_39  135114.0  5.428902e-01  0.133220 -0.051823  4.430029e-01   \n",
      "Landmark_40  135114.0  4.804765e-01  0.120692 -0.208993  3.968494e-01   \n",
      "Landmark_41  135114.0 -4.870579e-02  0.073999 -0.568066 -9.115915e-02   \n",
      "Landmark_42  135114.0  5.074212e-01  0.147115 -0.192790  3.976424e-01   \n",
      "Landmark_43  135114.0  4.045266e-01  0.129053 -0.428727  3.214576e-01   \n",
      "Landmark_44  135114.0 -1.095977e-01  0.120113 -0.732391 -1.801472e-01   \n",
      "Landmark_45  135114.0  4.908062e-01  0.136063 -0.105145  3.928790e-01   \n",
      "Landmark_46  135114.0  4.361518e-01  0.140571 -0.457718  3.271213e-01   \n",
      "Landmark_47  135114.0 -1.136662e-01  0.122067 -0.729435 -1.835648e-01   \n",
      "Landmark_48  135114.0  4.896273e-01  0.137227 -0.143479  3.967858e-01   \n",
      "Landmark_49  135114.0  4.580151e-01  0.165298 -0.612925  3.417088e-01   \n",
      "Landmark_50  135114.0 -1.012104e-01  0.118148 -0.829447 -1.677608e-01   \n",
      "Landmark_51  135114.0  5.819389e-01  0.187987 -0.182130  4.368811e-01   \n",
      "Landmark_52  135114.0  5.215235e-01  0.130780 -0.231939  4.291990e-01   \n",
      "Landmark_53  135114.0 -6.744088e-02  0.097214 -0.608679 -1.309672e-01   \n",
      "Landmark_54  135114.0  5.481208e-01  0.189962 -0.333142  4.081163e-01   \n",
      "Landmark_55  135114.0  4.624911e-01  0.137025 -0.413109  3.644253e-01   \n",
      "Landmark_56  135114.0 -1.076558e-01  0.128410 -0.751907 -1.870707e-01   \n",
      "Landmark_57  135114.0  5.324694e-01  0.175474 -0.243441  4.108955e-01   \n",
      "Landmark_58  135114.0  4.758410e-01  0.151183 -0.547213  3.724051e-01   \n",
      "Landmark_59  135114.0 -1.015641e-01  0.126652 -0.726823 -1.789146e-01   \n",
      "Landmark_60  135114.0  5.308556e-01  0.173944 -0.129397  4.173207e-01   \n",
      "Landmark_61  135114.0  4.870848e-01  0.172613 -0.651351  3.610032e-01   \n",
      "Landmark_62  135114.0 -8.709609e-02  0.122923 -0.713866 -1.613704e-01   \n",
      "\n",
      "                      50%           75%       max  \n",
      "Landmark_0   5.421198e-01  6.142133e-01  1.277888  \n",
      "Landmark_1   7.723594e-01  8.732420e-01  1.749022  \n",
      "Landmark_2   5.698978e-08  7.488931e-07  0.000005  \n",
      "Landmark_3   4.583854e-01  5.725022e-01  1.037628  \n",
      "Landmark_4   6.951036e-01  7.856820e-01  1.514191  \n",
      "Landmark_5  -4.750321e-02 -2.263098e-02  0.406063  \n",
      "Landmark_6   3.957292e-01  5.660803e-01  1.032584  \n",
      "Landmark_7   5.793300e-01  6.690112e-01  1.314783  \n",
      "Landmark_8  -7.306258e-02 -3.622872e-02  0.713092  \n",
      "Landmark_9   3.972637e-01  5.471052e-01  1.033270  \n",
      "Landmark_10  4.909196e-01  5.827274e-01  1.402053  \n",
      "Landmark_11 -9.925138e-02 -5.052667e-02  0.977298  \n",
      "Landmark_12  4.174276e-01  5.539937e-01  1.082134  \n",
      "Landmark_13  4.377341e-01  5.309650e-01  1.462351  \n",
      "Landmark_14 -1.189924e-01 -5.888496e-02  1.224705  \n",
      "Landmark_15  4.619344e-01  5.581336e-01  1.011305  \n",
      "Landmark_16  4.653773e-01  5.356653e-01  0.960281  \n",
      "Landmark_17 -2.463588e-02  1.446259e-02  0.718210  \n",
      "Landmark_18  4.309050e-01  5.399275e-01  1.051406  \n",
      "Landmark_19  3.401439e-01  4.115228e-01  0.979857  \n",
      "Landmark_20 -8.125081e-02 -3.325224e-02  1.095000  \n",
      "Landmark_21  4.097443e-01  5.327393e-01  1.219108  \n",
      "Landmark_22  2.976778e-01  3.819064e-01  1.230555  \n",
      "Landmark_23 -1.169741e-01 -5.616683e-02  1.220323  \n",
      "Landmark_24  3.996693e-01  5.347764e-01  1.355284  \n",
      "Landmark_25  2.899442e-01  3.966626e-01  1.511685  \n",
      "Landmark_26 -1.341526e-01 -6.648895e-02  1.276580  \n",
      "Landmark_27  5.038436e-01  5.697703e-01  1.003754  \n",
      "Landmark_28  4.721359e-01  5.388297e-01  0.960072  \n",
      "Landmark_29 -2.973857e-02  2.304216e-03  0.668671  \n",
      "Landmark_30  4.716639e-01  5.501309e-01  0.993028  \n",
      "Landmark_31  3.531036e-01  4.178183e-01  0.979425  \n",
      "Landmark_32 -9.044258e-02 -3.830394e-02  1.066684  \n",
      "Landmark_33  4.523817e-01  5.409128e-01  0.992831  \n",
      "Landmark_34  3.428809e-01  4.466080e-01  1.260457  \n",
      "Landmark_35 -1.175410e-01 -5.518279e-02  1.095731  \n",
      "Landmark_36  4.502347e-01  5.407484e-01  1.074430  \n",
      "Landmark_37  3.636736e-01  4.911988e-01  1.561810  \n",
      "Landmark_38 -1.232299e-01 -5.662101e-02  1.176661  \n",
      "Landmark_39  5.338707e-01  6.427095e-01  1.032744  \n",
      "Landmark_40  4.984795e-01  5.680661e-01  0.988888  \n",
      "Landmark_41 -4.512233e-02 -8.557926e-03  0.711743  \n",
      "Landmark_42  4.888577e-01  6.068959e-01  1.079354  \n",
      "Landmark_43  4.108122e-01  4.847230e-01  0.974996  \n",
      "Landmark_44 -1.077899e-01 -4.527557e-02  1.018012  \n",
      "Landmark_45  4.777749e-01  5.790667e-01  1.104201  \n",
      "Landmark_46  4.336722e-01  5.391092e-01  1.112201  \n",
      "Landmark_47 -1.124801e-01 -4.913994e-02  1.080275  \n",
      "Landmark_48  4.798374e-01  5.723210e-01  1.104138  \n",
      "Landmark_49  4.712831e-01  5.797937e-01  1.159950  \n",
      "Landmark_50 -9.604716e-02 -3.492600e-02  1.114676  \n",
      "Landmark_51  5.605884e-01  7.270595e-01  1.177035  \n",
      "Landmark_52  5.417402e-01  6.153260e-01  1.178985  \n",
      "Landmark_53 -6.881232e-02 -1.653726e-02  0.813254  \n",
      "Landmark_54  5.160871e-01  6.823608e-01  1.206926  \n",
      "Landmark_55  4.724384e-01  5.535670e-01  1.173611  \n",
      "Landmark_56 -1.096086e-01 -4.084451e-02  1.086744  \n",
      "Landmark_57  5.066389e-01  6.436592e-01  1.180199  \n",
      "Landmark_58  4.771359e-01  5.850132e-01  1.177672  \n",
      "Landmark_59 -1.058288e-01 -3.758821e-02  1.149530  \n",
      "Landmark_60  5.094443e-01  6.386584e-01  1.173651  \n",
      "Landmark_61  5.006344e-01  6.161844e-01  1.178016  \n",
      "Landmark_62 -9.032034e-02 -2.355776e-02  1.171922  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the transpose of the describe DataFrame\n",
    "print(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2925a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62f5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfle = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aacc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfle.Label = le.fit_transform(dfle.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8da9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfle.drop('Label', axis=1)\n",
    "y = dfle['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff41187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ba9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff1252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=100, xgb__learning_rate=0.01;, score=0.943 total time= 4.7min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=100, xgb__learning_rate=0.01;, score=0.944 total time= 4.7min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=100, xgb__learning_rate=0.1;, score=0.960 total time= 4.7min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=100, xgb__learning_rate=0.1;, score=0.961 total time= 4.7min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=100, xgb__learning_rate=0.3;, score=0.956 total time= 4.6min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=100, xgb__learning_rate=0.3;, score=0.956 total time= 4.6min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=200, xgb__learning_rate=0.01;, score=0.944 total time= 5.8min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=200, xgb__learning_rate=0.01;, score=0.945 total time= 5.9min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=200, xgb__learning_rate=0.1;, score=0.961 total time= 5.8min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=200, xgb__learning_rate=0.1;, score=0.961 total time= 6.0min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=200, xgb__learning_rate=0.3;, score=0.955 total time= 5.8min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=200, xgb__learning_rate=0.3;, score=0.955 total time= 5.8min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=300, xgb__learning_rate=0.01;, score=0.943 total time= 7.0min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=300, xgb__learning_rate=0.01;, score=0.946 total time= 7.0min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=300, xgb__learning_rate=0.1;, score=0.961 total time= 7.0min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=300, xgb__learning_rate=0.1;, score=0.961 total time= 7.0min\n",
      "[CV 1/2] END gb__n_estimators=2, rf__n_estimators=300, xgb__learning_rate=0.3;, score=0.955 total time= 6.9min\n",
      "[CV 2/2] END gb__n_estimators=2, rf__n_estimators=300, xgb__learning_rate=0.3;, score=0.956 total time= 6.9min\n",
      "[CV 1/2] END gb__n_estimators=4, rf__n_estimators=100, xgb__learning_rate=0.01;, score=0.941 total time= 7.9min\n",
      "[CV 2/2] END gb__n_estimators=4, rf__n_estimators=100, xgb__learning_rate=0.01;, score=0.943 total time= 7.7min\n",
      "[CV 1/2] END gb__n_estimators=4, rf__n_estimators=100, xgb__learning_rate=0.1;, score=0.960 total time= 7.8min\n",
      "[CV 2/2] END gb__n_estimators=4, rf__n_estimators=100, xgb__learning_rate=0.1;, score=0.961 total time= 7.8min\n",
      "[CV 1/2] END gb__n_estimators=4, rf__n_estimators=100, xgb__learning_rate=0.3;, score=0.955 total time= 7.6min\n",
      "[CV 2/2] END gb__n_estimators=4, rf__n_estimators=100, xgb__learning_rate=0.3;, score=0.955 total time= 7.6min\n",
      "[CV 1/2] END gb__n_estimators=4, rf__n_estimators=200, xgb__learning_rate=0.01;, score=0.941 total time= 8.8min\n",
      "[CV 2/2] END gb__n_estimators=4, rf__n_estimators=200, xgb__learning_rate=0.01;, score=0.943 total time= 8.8min\n",
      "[CV 1/2] END gb__n_estimators=4, rf__n_estimators=200, xgb__learning_rate=0.1;, score=0.960 total time= 8.8min\n",
      "[CV 2/2] END gb__n_estimators=4, rf__n_estimators=200, xgb__learning_rate=0.1;, score=0.961 total time= 8.8min\n"
     ]
    }
   ],
   "source": [
    "# Define individual models\n",
    "base_models = [\n",
    "    (\"rf\", RandomForestClassifier()),\n",
    "    (\"xgb\", XGBClassifier()),\n",
    "    (\"gb\", GradientBoostingClassifier()),\n",
    "]\n",
    "\n",
    "# Voting classifier with hard voting (majority wins)\n",
    "voting_clf = VotingClassifier(estimators=base_models, voting=\"soft\")\n",
    "\n",
    "# Stacking classifier with XGBoost as meta-learner\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=XGBClassifier())\n",
    "\n",
    "# GridSearchCV for parameter tuning (optional)\n",
    "# Replace with specific parameters for each model\n",
    "param_grid = {\n",
    "    \"rf__n_estimators\": [100, 200, 300],\n",
    "    \"xgb__learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"gb__n_estimators\": [2, 4, 6],\n",
    "}\n",
    "grid_search_voting = GridSearchCV(voting_clf, param_grid, cv=2, scoring=\"accuracy\",verbose = 5)\n",
    "grid_search_stacking = GridSearchCV(stacking_clf, param_grid, cv=2, scoring=\"accuracy\", verbose = 5)\n",
    "\n",
    "# Choose which ensemble method to train and evaluate (comment in/out as needed)\n",
    "grid_search_voting.fit(X_train, y_train)\n",
    "voting_clf = grid_search_voting.best_estimator_\n",
    "best_params_voting = grid_search_voting.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cbdb1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4672\\2843331910.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid_search_stacking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstacking_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search_stacking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbest_params_stacking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search_stacking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Print best parameters and score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"final_estimator_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m# base estimators will be used in transform, predict, and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# predict_proba. They are exposed publicly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_fit_single_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    451\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search_stacking.fit(X_train, y_train)\n",
    "stacking_clf = grid_search_stacking.best_estimator_\n",
    "best_params_stacking = grid_search_stacking.best_params_\n",
    "\n",
    "# Print best parameters and score\n",
    "#print(f\"Voting Ensemble - Best parameters: {best_params_voting}\")\n",
    "#print(f\"Voting Ensemble - Best score: {grid_search_voting.best_score_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ed3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stacking Ensemble - Best parameters: {best_params_stacking}\")\n",
    "print(f\"Stacking Ensemble - Best score: {grid_search_stacking.best_score_}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "#y_pred_voting = voting_clf.predict(X_test)\n",
    "#accuracy_voting = voting_clf.score(X_test, y_test)\n",
    "\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "accuracy_stacking = stacking_clf.score(X_test, y_test)\n",
    "\n",
    "#print(f\"Voting Ensemble Test accuracy: {accuracy_voting}\")\n",
    "\n",
    "print(f\"Stacking Ensemble Test accuracy: {accuracy_stacking}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
